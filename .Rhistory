bss_fit_AIC = bestglm(Final_Breast, family=binomial, IC="AIC")
bss_fit_BIC = bestglm(Final_Breast, family=binomial, IC="BIC")
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
## Identify best-fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
## Create multi-panel plotting device
par(mfrow=c(2,2))
## Produce plots, highlighting optimal value of k
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
pstar = 6
## Check which predictors are in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
## Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:(p+1)]))
Breast_subset = data.frame(X1[,indices], y)
## Obtain regression coefficients for this model
logreg1_fit = glm(y ~ ., data=Breast_subset, family="binomial")
summary(logreg1_fit)
## Set the seed to make the analysis reproducible
set.seed(10)
nfolds = 10
## Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
## Print first few fold-assignments
head(fold_index)
#Function to estimate the average MSE by general K-fold validation
reg_cv = function(X1, y, fold_ind) {
Xy = data.frame(X1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for(fold in 1:nfolds) {
glm_fit = glm(y ~ ., data=Xy[fold_ind!=fold,], family = binomial)
phat = predict(glm_fit, Xy[fold_ind==fold,], type= "response")
yhat = ifelse(phat > 0.5, 1 ,0)
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for(fold in 1:nfolds){
fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
}
reg_cv(Breast_subset[,1:6], Breast_subset$y, fold_index)
library(glmnet)
#Grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(X1, y, alpha=0,family="binomial", standardize=FALSE, lambda=grid )
#Plotting
plot(ridge_fit, xvar = "lambda", col=1:10, label= TRUE)
#Testing the lambda values
ridgefull_cv = cv.glmnet(X1, y, alpha=0, standardize=FALSE, lambda=grid, nfolds = nfolds, family = "binomial", type.measure = "class" , foldid = fold_index)
#Plotting how the cross-validated error varies with lambda
plot(ridgefull_cv)
#Finding the best lambda value
(lambda_min = ridgefull_cv$lambda.min)
## Set the seed to make the analysis reproducible
set.seed(1)
nfolds = 10
## Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
## Print first few fold-assignments
head(fold_index)
#Function to estimate the average MSE by general K-fold validation
reg_cv = function(X1, y, fold_ind) {
Xy = data.frame(X1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for(fold in 1:nfolds) {
glm_fit = glm(y ~ ., data=Xy[fold_ind!=fold,], family = binomial)
phat = predict(glm_fit, Xy[fold_ind==fold,], type= "response")
yhat = ifelse(phat > 0.5, 1 ,0)
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for(fold in 1:nfolds){
fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
}
reg_cv(Breast_subset[,1:6], Breast_subset$y, fold_index)
library(glmnet)
#Grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(X1, y, alpha=0,family="binomial", standardize=FALSE, lambda=grid )
#Plotting
plot(ridge_fit, xvar = "lambda", col=1:10, label= TRUE)
#Testing the lambda values
ridgefull_cv = cv.glmnet(X1, y, alpha=0, standardize=FALSE, lambda=grid, nfolds = nfolds, family = "binomial", type.measure = "class" , foldid = fold_index)
#Plotting how the cross-validated error varies with lambda
plot(ridgefull_cv)
#Finding the best lambda value
(lambda_min = ridgefull_cv$lambda.min)
lambda_min
#Extract which tuning parameter was the minimum
(i = which(ridgefull_cv$lambda == ridgefull_cv$lambda.min))
#The corresponding mean MSE
ridgefull_cv$cvm[i]
#The regression coefficients computed by performing ridge regression
coef(ridge_fit, s=lambda_min)
#Lasso
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X1, y, alpha=1, family="binomial", standardize=FALSE, lambda=grid)
#Plotting how the cross-validated error varies with lambda
plot(lasso_fit, xvar="lambda", col=rainbow(p), label=TRUE)
lasso_cv_fit = cv.glmnet(X1, y, alpha=1 , family="binomial", standardize=FALSE, lambda=grid ,type.measure="class", nfolds = nfolds,  foldid = fold_index  )
#Plot the cross-validation
plot(lasso_cv_fit)
## Extract the optimal value of the tuning parameter
(lambda_min = lasso_cv_fit$lambda.min)
## Which tuning parameter was the minimum?
(i = which(lasso_cv_fit$lambda == lasso_cv_fit$lambda.min))
## Extract corresponding mean MSE
lasso_cv_fit$cvm[i]
#Extract the Lasso regression coefficients
coef(lasso_fit, s=lambda_min)
#Lda
library(nclSLR)
(lda_fit = linDA(variables=Final_Breast[,c(1:8)], group=Final_Breast$y))
library(MASS)
Lda_model= lda(y~ ., data = Final_Breast)
Lda_model
#Applying 10 fold function to compute the test error for LDA
reg_cv_lda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = lda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$class
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
#Aplying the function to the data set and to receive the test error
reg_cv_lda(ExVars[,1:9], Final_Breast$y, fold_index)
#Qda
#Perform QDA on the training data
(qda_fit = quaDA(variables=Final_Breast[,c(1:8)], group=Final_Breast$y))
#Performing qda to recieve the group means
Qda_model= qda(y~ ., data = Final_Breast)
Qda_model
#Applying 10 fold function to compute the test error for QDA
reg_cv_qda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = qda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$class
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
#Aplying the function to the data set and to receive the test error
reg_cv_qda(ExVars[,1:9], Final_Breast$y, fold_index)
#Project
install.packages("mlbench")
install.packages("mlbench")
install.packages("foreach")
## Load mlbench package
library(mlbench)
## Load the data
data(BreastCancer)
View(BreastCancer)
?BreastCancer
## Check size
dim(BreastCancer)
head(BreastCancer)
#Spotting an NA row
BreastCancer[24,]
is.na(BreastCancer[24,])
#Removing all the NA row from the dataset
BreastCancer = na.omit(BreastCancer)#Removing all the NA row from the dataset
#Extracting explanatory variables
ExVars = BreastCancer[,2:10]
#Converting the explanatory variables from factors to numeric
indx <- sapply(ExVars, is.factor)
ExVars[indx] <- lapply(ExVars[indx], function(x) as.numeric(as.character(x)))
str(ExVars)
#Converting and extracting the response variable to 0 and 1
breast_y = data.frame(ExVars ,Class=as.integer(BreastCancer$Class)-1)
str(breast_y)
resp=breast_y$Class
resp
#Creating a new data frame with explanatory variable and rensopnse
MyBreast_data= data.frame(ExVars,resp)
#View how the observations are split into the 2 classes
table(MyBreast_data$resp)
#Plotting producing a pair plot of the predictors
pairs(MyBreast_data[,1:9], col=MyBreast_data[,10]+1)
#Compute the sample correlation matrix
cor(MyBreast_data)
#Extracting the predictor variables
Xraw = MyBreast_data[,1:9]
X1 = scale(Xraw)
#Extract response variable
y = MyBreast_data[,10]
y = as.factor(y)
#Combine the standarized var with the response in a new data frame
Final_Breast = data.frame(X1,y)
## Store n and p
n = nrow(Final_Breast); p = ncol(Final_Breast) - 1
#Fitting logistic regression
lg_fit = glm(y ~ ., data=Final_Breast, family="binomial")
summary(lg_fit)
## Load the bestglm package
library(bestglm)
library(leaps)
## Apply best subset selection
bss_fit_AIC = bestglm(Final_Breast, family=binomial, IC="AIC")
bss_fit_BIC = bestglm(Final_Breast, family=binomial, IC="BIC")
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
## Identify best-fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
## Create multi-panel plotting device
par(mfrow=c(2,2))
## Produce plots, highlighting optimal value of k
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
pstar = 6
## Check which predictors are in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
## Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:(p+1)]))
Breast_subset = data.frame(X1[,indices], y)
## Obtain regression coefficients for this model
logreg1_fit = glm(y ~ ., data=Breast_subset, family="binomial")
summary(logreg1_fit)
## Set the seed to make the analysis reproducible
set.seed(1)
nfolds = 10
## Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
## Print first few fold-assignments
head(fold_index)
#Function to estimate the average MSE by general K-fold validation
reg_cv = function(X1, y, fold_ind) {
Xy = data.frame(X1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for(fold in 1:nfolds) {
glm_fit = glm(y ~ ., data=Xy[fold_ind!=fold,], family = binomial)
phat = predict(glm_fit, Xy[fold_ind==fold,], type= "response")
yhat = ifelse(phat > 0.5, 1 ,0)
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for(fold in 1:nfolds){
fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
}
reg_cv(Breast_subset[,1:6], Breast_subset$y, fold_index)
library(glmnet)
#Grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(X1, y, alpha=0,family="binomial", standardize=FALSE, lambda=grid )
#Plotting
plot(ridge_fit, xvar = "lambda", col=1:10, label= TRUE)
#Testing the lambda values
ridgefull_cv = cv.glmnet(X1, y, alpha=0, standardize=FALSE, lambda=grid, nfolds = nfolds, family = "binomial", type.measure = "class" , foldid = fold_index)
#Plotting how the cross-validated error varies with lambda
plot(ridgefull_cv)
#Finding the best lambda value
(lambda_min = ridgefull_cv$lambda.min)
lambda_min
#Extract which tuning parameter was the minimum
(i = which(ridgefull_cv$lambda == ridgefull_cv$lambda.min))
#The corresponding mean MSE
ridgefull_cv$cvm[i]
#The regression coefficients computed by performing ridge regression
coef(ridge_fit, s=lambda_min)
#Lasso
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X1, y, alpha=1, family="binomial", standardize=FALSE, lambda=grid)
#Plotting how the cross-validated error varies with lambda
plot(lasso_fit, xvar="lambda", col=rainbow(p), label=TRUE)
lasso_cv_fit = cv.glmnet(X1, y, alpha=1 , family="binomial", standardize=FALSE, lambda=grid ,type.measure="class", nfolds = nfolds,  foldid = fold_index  )
#Plot the cross-validation
plot(lasso_cv_fit)
## Extract the optimal value of the tuning parameter
(lambda_min = lasso_cv_fit$lambda.min)
## Which tuning parameter was the minimum?
(i = which(lasso_cv_fit$lambda == lasso_cv_fit$lambda.min))
## Extract corresponding mean MSE
lasso_cv_fit$cvm[i]
#Extract the Lasso regression coefficients
coef(lasso_fit, s=lambda_min)
#Lda
library(nclSLR)
(lda_fit = linDA(variables=Final_Breast[,c(1:8)], group=Final_Breast$y))
library(MASS)
Lda_model= lda(y~ ., data = Final_Breast)
Lda_model
#Applying 10 fold function to compute the test error for LDA
reg_cv_lda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = lda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$class
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
#Aplying the function to the data set and to receive the test error
reg_cv_lda(ExVars[,1:9], Final_Breast$y, fold_index)
#Qda
#Perform QDA on the training data
(qda_fit = quaDA(variables=Final_Breast[,c(1:8)], group=Final_Breast$y))
#Performing qda to recieve the group means
Qda_model= qda(y~ ., data = Final_Breast)
Qda_model
#Applying 10 fold function to compute the test error for QDA
reg_cv_qda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = qda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$class
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
#Aplying the function to the data set and to receive the test error
reg_cv_qda(ExVars[,1:9], Final_Breast$y, fold_index)
#Project
install.packages("mlbench")
install.packages("foreach")
install.packages("mlbench")
## Load mlbench package
library(mlbench)
## Load the data
data(BreastCancer)
View(BreastCancer)
?BreastCancer
## Check size
dim(BreastCancer)
head(BreastCancer)
#Spotting an NA row
BreastCancer[24,]
is.na(BreastCancer[24,])
#Removing all the NA row from the dataset
BreastCancer = na.omit(BreastCancer)#Removing all the NA row from the dataset
#Extracting explanatory variables
ExVars = BreastCancer[,2:10]
#Converting the explanatory variables from factors to numeric
indx <- sapply(ExVars, is.factor)
ExVars[indx] <- lapply(ExVars[indx], function(x) as.numeric(as.character(x)))
str(ExVars)
#Converting and extracting the response variable to 0 and 1
breast_y = data.frame(ExVars ,Class=as.integer(BreastCancer$Class)-1)
str(breast_y)
resp=breast_y$Class
resp
#Creating a new data frame with explanatory variable and rensopnse
MyBreast_data= data.frame(ExVars,resp)
#View how the observations are split into the 2 classes
table(MyBreast_data$resp)
#Plotting producing a pair plot of the predictors
pairs(MyBreast_data[,1:9], col=MyBreast_data[,10]+1)
#Compute the sample correlation matrix
cor(MyBreast_data)
#Extracting the predictor variables
Xraw = MyBreast_data[,1:9]
X1 = scale(Xraw)
#Extract response variable
y = MyBreast_data[,10]
y = as.factor(y)
#Combine the standarized var with the response in a new data frame
Final_Breast = data.frame(X1,y)
## Store n and p
n = nrow(Final_Breast); p = ncol(Final_Breast) - 1
#Fitting logistic regression
lg_fit = glm(y ~ ., data=Final_Breast, family="binomial")
summary(lg_fit)
## Load the bestglm package
library(bestglm)
library(leaps)
## Apply best subset selection
bss_fit_AIC = bestglm(Final_Breast, family=binomial, IC="AIC")
bss_fit_BIC = bestglm(Final_Breast, family=binomial, IC="BIC")
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
## Identify best-fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
## Create multi-panel plotting device
par(mfrow=c(2,2))
## Produce plots, highlighting optimal value of k
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
pstar = 6
## Check which predictors are in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
## Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:(p+1)]))
Breast_subset = data.frame(X1[,indices], y)
## Obtain regression coefficients for this model
logreg1_fit = glm(y ~ ., data=Breast_subset, family="binomial")
summary(logreg1_fit)
## Set the seed to make the analysis reproducible
set.seed(1)
nfolds = 10
## Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
## Print first few fold-assignments
head(fold_index)
#Function to estimate the average MSE by general K-fold validation
reg_cv = function(X1, y, fold_ind) {
Xy = data.frame(X1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for(fold in 1:nfolds) {
glm_fit = glm(y ~ ., data=Xy[fold_ind!=fold,], family = binomial)
phat = predict(glm_fit, Xy[fold_ind==fold,], type= "response")
yhat = ifelse(phat > 0.5, 1 ,0)
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for(fold in 1:nfolds){
fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
}
reg_cv(Breast_subset[,1:6], Breast_subset$y, fold_index)
library(glmnet)
#Grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(X1, y, alpha=0,family="binomial", standardize=FALSE, lambda=grid )
#Plotting
plot(ridge_fit, xvar = "lambda", col=1:10, label= TRUE)
#Testing the lambda values
ridgefull_cv = cv.glmnet(X1, y, alpha=0, standardize=FALSE, lambda=grid, nfolds = nfolds, family = "binomial", type.measure = "class" , foldid = fold_index)
#Plotting how the cross-validated error varies with lambda
plot(ridgefull_cv)
#Finding the best lambda value
(lambda_min = ridgefull_cv$lambda.min)
lambda_min
#Extract which tuning parameter was the minimum
(i = which(ridgefull_cv$lambda == ridgefull_cv$lambda.min))
#The corresponding mean MSE
ridgefull_cv$cvm[i]
#The regression coefficients computed by performing ridge regression
coef(ridge_fit, s=lambda_min)
#Lasso
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X1, y, alpha=1, family="binomial", standardize=FALSE, lambda=grid)
#Plotting how the cross-validated error varies with lambda
plot(lasso_fit, xvar="lambda", col=rainbow(p), label=TRUE)
lasso_cv_fit = cv.glmnet(X1, y, alpha=1 , family="binomial", standardize=FALSE, lambda=grid ,type.measure="class", nfolds = nfolds,  foldid = fold_index  )
load.project()
setwd("~/LearnProject")
load.project()
library("ProjectTemplate")
load.project()
# Check for NAs
sum(is.na(cyber.security.1_question.response))
sum(is.na(cyber.security.1_step.activity))
