reg_cv_lda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = lda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$class
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
#Aplying the function to the data set and to receive the test error
reg_cv_lda(ExVars[,1:9], Final_Breast$y, fold_index)
#Qda
#Perform QDA on the training data
(qda_fit = quaDA(variables=Final_Breast[,c(1:8)], group=Final_Breast$y))
#Performing qda to recieve the group means
Qda_model= qda(y~ ., data = Final_Breast)
Qda_model
#Applying 10 fold function to compute the test error for QDA
reg_cv_qda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = qda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$class
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
#Aplying the function to the data set and to receive the test error
reg_cv_qda(ExVars[,1:9], Final_Breast$y, fold_index)
#Project
install.packages("mlbench")
install.packages("mlbench")
install.packages("foreach")
## Load mlbench package
library(mlbench)
## Load the data
data(BreastCancer)
View(BreastCancer)
?BreastCancer
## Check size
dim(BreastCancer)
head(BreastCancer)
#Spotting an NA row
BreastCancer[24,]
is.na(BreastCancer[24,])
#Removing all the NA row from the dataset
BreastCancer = na.omit(BreastCancer)#Removing all the NA row from the dataset
#Extracting explanatory variables
ExVars = BreastCancer[,2:10]
#Converting the explanatory variables from factors to numeric
indx <- sapply(ExVars, is.factor)
ExVars[indx] <- lapply(ExVars[indx], function(x) as.numeric(as.character(x)))
str(ExVars)
#Converting and extracting the response variable to 0 and 1
breast_y = data.frame(ExVars ,Class=as.integer(BreastCancer$Class)-1)
str(breast_y)
resp=breast_y$Class
resp
#Creating a new data frame with explanatory variable and rensopnse
MyBreast_data= data.frame(ExVars,resp)
#View how the observations are split into the 2 classes
table(MyBreast_data$resp)
#Plotting producing a pair plot of the predictors
pairs(MyBreast_data[,1:9], col=MyBreast_data[,10]+1)
#Compute the sample correlation matrix
cor(MyBreast_data)
#Extracting the predictor variables
Xraw = MyBreast_data[,1:9]
X1 = scale(Xraw)
#Extract response variable
y = MyBreast_data[,10]
y = as.factor(y)
#Combine the standarized var with the response in a new data frame
Final_Breast = data.frame(X1,y)
## Store n and p
n = nrow(Final_Breast); p = ncol(Final_Breast) - 1
#Fitting logistic regression
lg_fit = glm(y ~ ., data=Final_Breast, family="binomial")
summary(lg_fit)
## Load the bestglm package
library(bestglm)
library(leaps)
## Apply best subset selection
bss_fit_AIC = bestglm(Final_Breast, family=binomial, IC="AIC")
bss_fit_BIC = bestglm(Final_Breast, family=binomial, IC="BIC")
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
## Identify best-fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
## Create multi-panel plotting device
par(mfrow=c(2,2))
## Produce plots, highlighting optimal value of k
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
pstar = 6
## Check which predictors are in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
## Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:(p+1)]))
Breast_subset = data.frame(X1[,indices], y)
## Obtain regression coefficients for this model
logreg1_fit = glm(y ~ ., data=Breast_subset, family="binomial")
summary(logreg1_fit)
## Set the seed to make the analysis reproducible
set.seed(1)
nfolds = 10
## Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
## Print first few fold-assignments
head(fold_index)
#Function to estimate the average MSE by general K-fold validation
reg_cv = function(X1, y, fold_ind) {
Xy = data.frame(X1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for(fold in 1:nfolds) {
glm_fit = glm(y ~ ., data=Xy[fold_ind!=fold,], family = binomial)
phat = predict(glm_fit, Xy[fold_ind==fold,], type= "response")
yhat = ifelse(phat > 0.5, 1 ,0)
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for(fold in 1:nfolds){
fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
}
reg_cv(Breast_subset[,1:6], Breast_subset$y, fold_index)
library(glmnet)
#Grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(X1, y, alpha=0,family="binomial", standardize=FALSE, lambda=grid )
#Plotting
plot(ridge_fit, xvar = "lambda", col=1:10, label= TRUE)
#Testing the lambda values
ridgefull_cv = cv.glmnet(X1, y, alpha=0, standardize=FALSE, lambda=grid, nfolds = nfolds, family = "binomial", type.measure = "class" , foldid = fold_index)
#Plotting how the cross-validated error varies with lambda
plot(ridgefull_cv)
#Finding the best lambda value
(lambda_min = ridgefull_cv$lambda.min)
lambda_min
#Extract which tuning parameter was the minimum
(i = which(ridgefull_cv$lambda == ridgefull_cv$lambda.min))
#The corresponding mean MSE
ridgefull_cv$cvm[i]
#The regression coefficients computed by performing ridge regression
coef(ridge_fit, s=lambda_min)
#Lasso
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X1, y, alpha=1, family="binomial", standardize=FALSE, lambda=grid)
#Plotting how the cross-validated error varies with lambda
plot(lasso_fit, xvar="lambda", col=rainbow(p), label=TRUE)
lasso_cv_fit = cv.glmnet(X1, y, alpha=1 , family="binomial", standardize=FALSE, lambda=grid ,type.measure="class", nfolds = nfolds,  foldid = fold_index  )
#Plot the cross-validation
plot(lasso_cv_fit)
## Extract the optimal value of the tuning parameter
(lambda_min = lasso_cv_fit$lambda.min)
## Which tuning parameter was the minimum?
(i = which(lasso_cv_fit$lambda == lasso_cv_fit$lambda.min))
## Extract corresponding mean MSE
lasso_cv_fit$cvm[i]
#Extract the Lasso regression coefficients
coef(lasso_fit, s=lambda_min)
#Lda
library(nclSLR)
(lda_fit = linDA(variables=Final_Breast[,c(1:8)], group=Final_Breast$y))
library(MASS)
Lda_model= lda(y~ ., data = Final_Breast)
Lda_model
#Applying 10 fold function to compute the test error for LDA
reg_cv_lda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = lda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$class
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
#Aplying the function to the data set and to receive the test error
reg_cv_lda(ExVars[,1:9], Final_Breast$y, fold_index)
#Qda
#Perform QDA on the training data
(qda_fit = quaDA(variables=Final_Breast[,c(1:8)], group=Final_Breast$y))
#Performing qda to recieve the group means
Qda_model= qda(y~ ., data = Final_Breast)
Qda_model
#Applying 10 fold function to compute the test error for QDA
reg_cv_qda = function(x1, y, fold_ind){
Xy = data.frame(x1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for (fold in 1:nfolds) {
tmp_fit = qda(y~., data = Xy[fold_ind!=fold,])
phat = predict(tmp_fit, Xy[fold_ind == fold,])
yhat = phat$class
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for (fold in 1:nfolds) fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
#Aplying the function to the data set and to receive the test error
reg_cv_qda(ExVars[,1:9], Final_Breast$y, fold_index)
#Project
install.packages("mlbench")
install.packages("foreach")
install.packages("mlbench")
## Load mlbench package
library(mlbench)
## Load the data
data(BreastCancer)
View(BreastCancer)
?BreastCancer
## Check size
dim(BreastCancer)
head(BreastCancer)
#Spotting an NA row
BreastCancer[24,]
is.na(BreastCancer[24,])
#Removing all the NA row from the dataset
BreastCancer = na.omit(BreastCancer)#Removing all the NA row from the dataset
#Extracting explanatory variables
ExVars = BreastCancer[,2:10]
#Converting the explanatory variables from factors to numeric
indx <- sapply(ExVars, is.factor)
ExVars[indx] <- lapply(ExVars[indx], function(x) as.numeric(as.character(x)))
str(ExVars)
#Converting and extracting the response variable to 0 and 1
breast_y = data.frame(ExVars ,Class=as.integer(BreastCancer$Class)-1)
str(breast_y)
resp=breast_y$Class
resp
#Creating a new data frame with explanatory variable and rensopnse
MyBreast_data= data.frame(ExVars,resp)
#View how the observations are split into the 2 classes
table(MyBreast_data$resp)
#Plotting producing a pair plot of the predictors
pairs(MyBreast_data[,1:9], col=MyBreast_data[,10]+1)
#Compute the sample correlation matrix
cor(MyBreast_data)
#Extracting the predictor variables
Xraw = MyBreast_data[,1:9]
X1 = scale(Xraw)
#Extract response variable
y = MyBreast_data[,10]
y = as.factor(y)
#Combine the standarized var with the response in a new data frame
Final_Breast = data.frame(X1,y)
## Store n and p
n = nrow(Final_Breast); p = ncol(Final_Breast) - 1
#Fitting logistic regression
lg_fit = glm(y ~ ., data=Final_Breast, family="binomial")
summary(lg_fit)
## Load the bestglm package
library(bestglm)
library(leaps)
## Apply best subset selection
bss_fit_AIC = bestglm(Final_Breast, family=binomial, IC="AIC")
bss_fit_BIC = bestglm(Final_Breast, family=binomial, IC="BIC")
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
## Identify best-fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
## Create multi-panel plotting device
par(mfrow=c(2,2))
## Produce plots, highlighting optimal value of k
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
pstar = 6
## Check which predictors are in the 6-predictor model
bss_fit_AIC$Subsets[pstar+1,]
## Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:(p+1)]))
Breast_subset = data.frame(X1[,indices], y)
## Obtain regression coefficients for this model
logreg1_fit = glm(y ~ ., data=Breast_subset, family="binomial")
summary(logreg1_fit)
## Set the seed to make the analysis reproducible
set.seed(1)
nfolds = 10
## Sample fold-assignment index
fold_index = sample(nfolds, n, replace=TRUE)
## Print first few fold-assignments
head(fold_index)
#Function to estimate the average MSE by general K-fold validation
reg_cv = function(X1, y, fold_ind) {
Xy = data.frame(X1, y=y)
nfolds = max(fold_ind)
if(!all.equal(sort(unique(fold_ind)), 1:nfolds)) stop("Invalid fold partition.")
cv_errors = numeric(nfolds)
for(fold in 1:nfolds) {
glm_fit = glm(y ~ ., data=Xy[fold_ind!=fold,], family = binomial)
phat = predict(glm_fit, Xy[fold_ind==fold,], type= "response")
yhat = ifelse(phat > 0.5, 1 ,0)
yobs = y[fold_ind==fold]
cv_errors[fold] = 1 - mean(yobs == yhat)
}
fold_sizes = numeric(nfolds)
for(fold in 1:nfolds){
fold_sizes[fold] = length(which(fold_ind==fold))
test_error = weighted.mean(cv_errors, w=fold_sizes)
return(test_error)
}
}
reg_cv(Breast_subset[,1:6], Breast_subset$y, fold_index)
library(glmnet)
#Grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(X1, y, alpha=0,family="binomial", standardize=FALSE, lambda=grid )
#Plotting
plot(ridge_fit, xvar = "lambda", col=1:10, label= TRUE)
#Testing the lambda values
ridgefull_cv = cv.glmnet(X1, y, alpha=0, standardize=FALSE, lambda=grid, nfolds = nfolds, family = "binomial", type.measure = "class" , foldid = fold_index)
#Plotting how the cross-validated error varies with lambda
plot(ridgefull_cv)
#Finding the best lambda value
(lambda_min = ridgefull_cv$lambda.min)
lambda_min
#Extract which tuning parameter was the minimum
(i = which(ridgefull_cv$lambda == ridgefull_cv$lambda.min))
#The corresponding mean MSE
ridgefull_cv$cvm[i]
#The regression coefficients computed by performing ridge regression
coef(ridge_fit, s=lambda_min)
#Lasso
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X1, y, alpha=1, family="binomial", standardize=FALSE, lambda=grid)
#Plotting how the cross-validated error varies with lambda
plot(lasso_fit, xvar="lambda", col=rainbow(p), label=TRUE)
lasso_cv_fit = cv.glmnet(X1, y, alpha=1 , family="binomial", standardize=FALSE, lambda=grid ,type.measure="class", nfolds = nfolds,  foldid = fold_index  )
setwd("~/FutureLearn")
library("ProjectTemplate")
load.project()
#Checking the number of people enrolled
dim(cyber.security.1_enrolments)
# Example preprocessing script.
enrollCount1 = unique(cyber.security.1_enrolments$learner_id)
enrollCount1
unique(cyber.security.1_enrolments$learner_id)
r
#Checking the number of people enrolled
dim(cyber.security.1_enrolments)
unique(cyber.security.1_enrolments$learner_id)
dim(enrollCount1)
View(enrollCount1)
# Example preprocessing script.
cyber.security.1_enrolments %>% filter_all(all_vars(!is.na(.)))
# Example preprocessing script.
library(dplyr)
cyber.security.1_enrolments %>% filter_all(all_vars(!is.na(.)))
cyber.security.1_enrolments %>% filter_all(all_vars(complete.cases(.)))
view(cyber.security.1_enrolments)
#Checking the number of people enrolled
dim(cyber.security.1_enrolments)
test1 = cyber.security.1_enrolments %>% filter_all(all_vars(!is.na(.)))
View(test1)
View(test1)
load.project()
load.project()
rm(test1)
rm(enrollCount1)
setwd("~/")
library("ProjectTemplate")
load.project()
create.project("LearnProject")
setwd("~/LearnProject/reports")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
# Load project
library("ProjectTemplate")
load.project()
setwd("~/LearnProject/src")
load.project()
load.project()
setwd("~/LearnProject/src")
load.project()
# Example preprocessing script.
distinct(cyber.security.1_enrolments,learner_id)
dim(cyber.security.1_enrolments)
filter(cyber.security.1_enrolments, fully_participated_at= !NUll)
filter(cyber.security.1_enrolments, fully_participated_at == !NUll)
filter(cyber.security.1_enrolments, fully_participated_at == !NaN)
fully_participated= filter(cyber.security.1_enrolments, fully_participated_at == !NaN)
fully_participated %>%
group_by(fully_participated_at)
fully_participated %>%
test1= group_by(fully_participated_at)
fully_participated %>%
test1= group_by(fully_participated_at)
fully_participated %>%
test1= group_by(fully_participated_at)
pull(cyber.security.1_enrolments,fully_participated_at)
?pull
# Example preprocessing script.
library(tidyverse)
test= cyber.security.1_enrolments %>% drop_na(fully_participated_at)
View(test)
test = cyber.security.1_enrolments [!(!is.na(cyber.security.1_enrolments$fully_participated_at) & cyber.security.1_enrolments$fully_participated_at==""), ]
View(test)
dim(test)
14394/1803*100
14394/1803
1803*100):14840
(1803*100):14840
(1803*100):14840
fully_participated= (1803*100)/14840
fully_participated
Fully_finished1 = cyber.security.1_enrolments [!(!is.na(cyber.security.1_enrolments$fully_participated_at) & cyber.security.1_enrolments$fully_participated_at==""), ]
Fully_finished2 = cyber.security.2_enrolments [!(!is.na(cyber.security.2_enrolments$fully_participated_at) & cyber.security.2_enrolments$fully_participated_at==""), ]
dim(cyber.security.1_enrolments)
dim(cyber.security.2_enrolments)
fully_participated2 = (33*100)/14840
fully_participated2 = (33*100)/6488
fully_participated1 = (1803*100)/14840
Fully_finished3 = cyber.security.3_enrolments [!(!is.na(cyber.security.3_enrolments$fully_participated_at) & cyber.security.3_enrolments$fully_participated_at==""), ]
dim(cyber.security.3_enrolments)
fully_participated2 = (56*100)/3361
fully_participated3 = (56*100)/3361
fully_participated2 = (33*100)/6488
dim(cyber.security.3_enrolments)
dim(cyber.security.4_enrolments)
Fully_finished4 = cyber.security.4_enrolments [!(!is.na(cyber.security.4_enrolments$fully_participated_at) & cyber.security.4_enrolments$fully_participated_at==""), ]
fully_participated4 = (166*100)/3992
Fully_finished5 = cyber.security.5_enrolments [!(!is.na(cyber.security.5_enrolments$fully_participated_at) & cyber.security.5_enrolments$fully_participated_at==""), ]
Fully_finished6 = cyber.security.6_enrolments [!(!is.na(cyber.security.6_enrolments$fully_participated_at) & cyber.security.6_enrolments$fully_participated_at==""), ]
Fully_finished7 = cyber.security.7_enrolments [!(!is.na(cyber.security.7_enrolments$fully_participated_at) & cyber.security.7_enrolments$fully_participated_at==""), ]
dim(cyber.security.5_enrolments)
dim(cyber.security.5_enrolments)
dim(cyber.security.6_enrolments)
dim(cyber.security.7_enrolments)
fully_participated5 = (22*100)/3544
fully_participated6 = (31*100)/3175
fully_participated7 = (43*100)/2342
View(cyber.security.1_enrolments)
filter(cyber.security.1_enrolments, gender = male)
filter(cyber.security.1_enrolments, gender == male)
mutate(cyber.security.1_enrolments, gender == male)
mutate(cyber.security.1_enrolments, gender == "male")
genders = mutate(cyber.security.1_enrolments, gender == "male")
View(genders)
genders = mutate(cyber.security.1_enrolments, gender = male)
?dplyr
?select
select(cyber.security.1_enrolments,contains(male))
select(cyber.security.1_enrolments,contains("male"))
select(cyber.security.1_enrolments$gender,contains("male"))
select(cyber.security.1_enrolments,genders)
cyber.security.1_enrolments %>% select (genders)
View(cyber.security.1_enrolments)
filter(cyber.security.1_enrolments, gender == "male")
filter(cyber.security.1_enrolments, gender == "females")
filter(cyber.security.1_enrolments, gender == "female")
filter(cyber.security.1_enrolments, gender == "unknown")
filter(cyber.security.1_enrolments, gender == "Unknown")
MaleLearners = filter(cyber.security.1_enrolments, gender == "male")
FemaleLearners filter(cyber.security.1_enrolments, gender == "female")
UnknownGenderLearners= filter(cyber.security.1_enrolments, gender == "Unknown")
FemaleLearners = filter(cyber.security.1_enrolments, gender == "female")
UnknownGenderLearners= filter(cyber.security.1_enrolments, gender == "Unknown")
View(cyber.security.1_enrolments)
distinct(cyber.security.1_enrolments, detected_country)
distinct(cyber.security.1_enrolments, highest_education_level)
univercityDegree = filter(cyber.security.1_enrolments, highest_education_level == "university_degree ")
View(univercityDegree)
univercityDegree = filter(cyber.security.1_enrolments, highest_education_level == "university_degree")
univercitydoctorate  = filter(cyber.security.1_enrolments, highest_education_level == "university_doctorate")
professional  = filter(cyber.security.1_enrolments, highest_education_level == "professional")
UnknownEducation = filter(cyber.security.1_enrolments, highest_education_level == "Unknown")
#Allocating education levels
distinct(cyber.security.1_enrolments, employment_status)
UnknownEmployment = filter(cyber.security.1_enrolments, employment_status == "Unknown")
setwd("~/LearnProject/munge")
View(cyber.security.1_question.response)
